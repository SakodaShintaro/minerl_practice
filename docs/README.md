# まとめノート

<https://tokumini.hatenablog.com/archive/category/MineRL> で毎週の進捗はまとめているが、全体としてどうなっているかはわかりづらいのでこのページでまとめる。

## 目標

このリポジトリで実装しているものの目標は、オンライン強化学習(ストリーミング強化学習)の設定におけるMineRLで、ある程度意味のある学習ができているエージェントを構築することである。

ストリーミング強化学習とは <https://arxiv.org/abs/2410.14606> で設定されているものであり、データが連続的にやってくる強化学習でそれらをリプレイバッファに貯めることなどをせずその場で学習する。

## 方針

世界モデルを学習し、内発報酬に利用する。

まず、MineRLの報酬は疎であるため、完全に報酬のみから学習を進めることは難しいのではないかという仮説がある。そのため、内発報酬を計算するために世界モデルを学習し、それの予測結果と実際の結果の差分からなにかしらの指標を出す。世界モデルは内発報酬のためだけでなく、いずれは意思決定のための探索にも使用するかもしれない。

世界モデル自体は潜在空間におけるフローマッチング <https://arxiv.org/abs/2210.02747> で学習する。世界の遷移はおそらく多数の不確実性があり、確率的なモデル化は必要だと考えられる。高速に生成できなければいけないので、1ステップで生成する際の結果を改善する工夫などを入れる必要があると考えている(<https://arxiv.org/abs/2410.12557>)。その他、世界モデル自体の学習の工夫も進めてみたい (<https://arxiv.org/abs/2401.17835>)。

内発報酬の計算の仕方はまだ全然決まっていないが、「行動で条件付けした予測結果と実際の状態の一致具合 - 条件付けなしでの予測結果と実際の状態の一致具合」などになるのではないか。単に「行動で条件付けした予測結果と実際の状態の一致具合」だけを使うと、それを最大化するには単に暴れていればいいだけに思われる。ランダムな画像を表示するだけのテレビが環境にある場合にそこに吸われるのではないか、という思考実験も気になる。(<https://arxiv.org/abs/1808.04355>におけるnoisy-TV Problem、あるいは<https://arxiv.org/abs/1810.02274>におけるcouch-potato issues)

## 現状の成果

ストリーミング設定でフローマッチングの学習が実行できるようにはなっている。まだ性能などは十分ではないが、最低限インベントリを開く行動に対応して画面が大きく変わることは予測できている。
