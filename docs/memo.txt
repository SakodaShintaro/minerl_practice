強化学習ありでの手順

- 時刻tが始まる
- 価値v_tを計算
- 行動a_tをサンプリング(対数確率も出す)
- 現状態f_tと行動a_tを使って次画像i_tを予測
- 環境を進めて観測i_{t+1}を得る
- 予測i_tと観測i_{t+1}を使って検証損失を計算
- ランダムノイズを生成してフローを計算し学習損失を計算
- 価値v_{t+1}を計算
- delta = r + \gamma v_{t+1} - v_t
- policy_lossを計算
- value_lossを計算